# -*- coding: utf-8 -*-
"""New Geo NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zB9ohx3WT97vwqfk3CR5gMT-z-vgYSPC
"""

#importing necessary libraries 
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
import csv

#getting the article title and body text
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
nltk.download("stopwords")
import string

def remove_punctuation(text):
    # Create a translation table with punctuation characters mapped to None
    translation_table = str.maketrans("", "", string.punctuation)
    
    # Remove punctuation using the translation table
    text_without_punctuation = text.translate(translation_table)
    
    return text_without_punctuation

# Example usage
sentence = "Hello, world! This is a sentence with punctuation."
cleaned_sentence = remove_punctuation(sentence)
print(cleaned_sentence)

def Cleamtext(url):
  # Replace with the URL of the article you want to scrape
  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')

  title_element = soup.find('h1', class_='headline')
  subtitle_element = soup.find('h2', class_='sub-headline speakable')
  article_div = soup.find('div', class_='article-body')


# Extract the text content of the title
  title = title_element.get_text()
  subtitle = subtitle_element.get_text()
  article_text = article_div.get_text()
  combined = title + ' ' + subtitle + ' ' + article_text

#cleaning the text from stopwords

  stopwords_ = set(stopwords.words("english"))
  
  rawtokens = combined.split()
  clean_tokens = [t for t in rawtokens if not t in stopwords_]
  clean_text = " ".join(clean_tokens)
  cleanertext = remove_punctuation(clean_text)

  import nltk
  nltk.download('punkt')
  from nltk.tokenize import sent_tokenize, word_tokenize

  tokens = word_tokenize(cleanertext)
  sentences = sent_tokenize(cleanertext)
  n = len(tokens)

  from nltk.sentiment import SentimentIntensityAnalyzer
  import nltk
  nltk.download('vader_lexicon')

# Instantiate the VADER sentiment analyzer
  sid = SentimentIntensityAnalyzer()

# Example sentence

# Get the sentiment scores

  sentiment_scores = sid.polarity_scores(cleanertext)
  positive_score = sentiment_scores['pos']
  negative_score = sentiment_scores['neg']
  neutral_score = sentiment_scores['neu']

# Print the sentiment scores
  print(sentiment_scores)

# Calculate subjectivity score
  subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)

# Print the computed subjectivity score
  print("Subjectivity Score:", subjectivity_score)

  return   sentiment_scores, subjectivity_score

input_csv_file = 'fox.csv'

# Output CSV file to store analysis results
output_csv_file = 'analysis_results.csv'

# List to store analysis results
analysis_results = []

# Open the input CSV file and read URLs line by line
with open(input_csv_file, 'r') as file:
    csv_reader = csv.reader(file)
    for row in csv_reader:
        url = row[0]  # Assuming the URL is in the first column
        positive, negative = Cleamtext(url)
        analysis_results.append((url, positive, negative))

# Write analysis results to the output CSV file
with open(output_csv_file, 'w', newline='') as file:
    csv_writer = csv.writer(file)
    csv_writer.writerow(['URL', 'Positive Score', 'Negative Score'])  # Write header
    csv_writer.writerows(analysis_results)
